{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70cd661-5e59-46f2-be73-53f6881cee6f",
   "metadata": {},
   "source": [
    "# Data Chunking\n",
    "\n",
    "#### This script converts the stock data into manageable data chunks and orders them on an yearly and quarterly bases in folders. The data can be saves as a csv file or alternatively as a pickle datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d5295-7707-466a-9b4e-1a43b2e74fba",
   "metadata": {},
   "source": [
    "Load the necessary python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ddb5ce2-356b-4b87-bd9f-f7fffc215d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57152e5e-729c-4175-bc0c-4fb8f3013c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Processing chunk 7...\n",
      "Processing chunk 8...\n",
      "Processing chunk 9...\n",
      "Processing chunk 10...\n",
      "Processing chunk 11...\n",
      "Processing chunk 12...\n",
      "Processing chunk 13...\n",
      "Processing chunk 14...\n",
      "Processing chunk 15...\n",
      "Processing chunk 16...\n",
      "Processing chunk 17...\n",
      "Processing chunk 18...\n",
      "Processing chunk 19...\n",
      "Processing chunk 20...\n",
      "Processing chunk 21...\n",
      "Processing chunk 22...\n",
      "Processing chunk 23...\n",
      "Processing chunk 24...\n",
      "Processing chunk 25...\n",
      "Processing chunk 26...\n",
      "Processing chunk 27...\n",
      "Processing chunk 28...\n",
      "Processing chunk 29...\n",
      "Processing chunk 30...\n",
      "Processing chunk 31...\n",
      "Processing chunk 32...\n",
      "Processing chunk 33...\n",
      "Processing chunk 34...\n",
      "Processing chunk 35...\n",
      "Processing chunk 36...\n",
      "Processing chunk 37...\n",
      "Processing chunk 38...\n",
      "Processing chunk 39...\n",
      "Processing chunk 40...\n",
      "Processing chunk 41...\n",
      "Processing chunk 42...\n",
      "Processing chunk 43...\n",
      "Processing chunk 44...\n",
      "Processing chunk 45...\n",
      "Processing chunk 46...\n",
      "Processing chunk 47...\n",
      "Processing chunk 48...\n",
      "Processing chunk 49...\n",
      "Processing chunk 50...\n",
      "Processing chunk 51...\n",
      "Processing chunk 52...\n",
      "Processing chunk 53...\n",
      "Processing chunk 54...\n",
      "Processing chunk 55...\n",
      "Processing chunk 56...\n",
      "Processing chunk 57...\n",
      "Processing chunk 58...\n",
      "Processing chunk 59...\n",
      "Processing chunk 60...\n",
      "Processing chunk 61...\n",
      "Processing chunk 62...\n",
      "Processing chunk 63...\n",
      "Processing chunk 64...\n",
      "Processing chunk 65...\n",
      "Processing chunk 66...\n",
      "Processing chunk 67...\n",
      "Processing chunk 68...\n",
      "Processing chunk 69...\n",
      "Processing chunk 70...\n",
      "Processing chunk 71...\n",
      "Processing chunk 72...\n",
      "Processing chunk 73...\n",
      "Processing chunk 74...\n",
      "Processing chunk 75...\n",
      "Processing chunk 76...\n",
      "Processing chunk 77...\n",
      "Processing chunk 78...\n",
      "Processing chunk 79...\n",
      "Processing chunk 80...\n",
      "Processing chunk 81...\n",
      "Processing chunk 82...\n",
      "Processing chunk 83...\n",
      "Processing chunk 84...\n",
      "Processing chunk 85...\n",
      "Processing chunk 86...\n",
      "Processing chunk 87...\n",
      "Processing chunk 88...\n",
      "Processing chunk 89...\n",
      "Processing chunk 90...\n",
      "Processing chunk 91...\n",
      "Processing chunk 92...\n",
      "Processing chunk 93...\n",
      "Processing chunk 94...\n",
      "Processing chunk 95...\n",
      "Processing chunk 96...\n",
      "Processing chunk 97...\n",
      "Processing chunk 98...\n",
      "Processing chunk 99...\n",
      "Processing chunk 100...\n",
      "Processing chunk 101...\n",
      "Processing chunk 102...\n",
      "Processing chunk 103...\n",
      "Processing chunk 104...\n",
      "Data processing and saving to Pickle files complete.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#title           :Stata_data_file_chunking.py\n",
    "#description     :This will create a header for a python script.\n",
    "#author          :Emmanuel Mensah Boateng\n",
    "#date            :20240915\n",
    "#version         :1.0\n",
    "#usage           :python pyscript.py\n",
    "#notes           :\n",
    "#==============================================================================\n",
    "\n",
    "\n",
    "# p<th to stata .dta file path for chunking\n",
    "file_path = '../Data/dsws_variables_monthly_basic_2021.dta'\n",
    "\n",
    "# Output directory for results \n",
    "output_dir = '../Data/dsws_Data/'  \n",
    "os.makedirs(output_dir, exist_ok=True)  # Create directory if nonexistent\n",
    "\n",
    "# Define chunk size (e.g., 100,000 rows at a time)\n",
    "chunk_size = 100000\n",
    "\n",
    "# Function to extract the year and quarter from the data\n",
    "def extract_year_quarter(df, date_column):\n",
    "    df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "    df['quarter'] = pd.to_datetime(df['date']).dt.to_period('Q')\n",
    "    \n",
    "    # Convert 'quarter' to a string to avoid issues\n",
    "    df['quarter'] = df['quarter'].astype(str)\n",
    "    \n",
    "    # Return the DataFrame with both columns at once to avoid fragmentation\n",
    "    return df\n",
    "\n",
    "# Function to extract the year and quarter from the data\n",
    "def extract_year_quarter(df, date_column):\n",
    "    # Use loc to avoid SettingWithCopyWarning and make a copy to avoid fragmentation\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create new columns in a single step to avoid DataFrame fragmentation\n",
    "    df['year'] = pd.to_datetime(df[date_column]).dt.year\n",
    "    df['quarter'] = pd.to_datetime(df[date_column]).dt.to_period('Q').astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Read the .dta file in chunks\n",
    "chunk_iter = pd.read_stata(file_path, chunksize=chunk_size)\n",
    "\n",
    "# Process each chunk\n",
    "for i, chunk in enumerate(chunk_iter):\n",
    "    print(f'Processing chunk {i+1}...')\n",
    "    \n",
    "    # Filter data where the 'country' column has value 'usa'\n",
    "    chunk = chunk.loc[chunk['country'] == 'usa']  # Use .loc[] to avoid SettingWithCopyWarning\n",
    "\n",
    "    # If no rows left after filtering, skip this chunk\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "    \n",
    "    # Extract year and quarter (assuming you have a date column like 'date')\n",
    "    chunk = extract_year_quarter(chunk, 'date')  # Update 'date' to your actual date column\n",
    "\n",
    "    # Group the chunk by year and quarter, and save it into separate Pickle files\n",
    "    for (year, quarter), group in chunk.groupby(['year', 'quarter']):\n",
    "        # Create a subdirectory for each year\n",
    "        year_dir = os.path.join(output_dir, str(year))\n",
    "        os.makedirs(year_dir, exist_ok=True)\n",
    "\n",
    "        # Define the filename for the Pickle file for each quarter\n",
    "        pickle_filename = os.path.join(year_dir, f'{year}_Q{quarter[-1]}.pkl')\n",
    "\n",
    "        # Save the group data to a Pickle file\n",
    "        group.to_pickle(pickle_filename)\n",
    "\n",
    "print(\"Data processing and saving to Pickle files complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383013f-9011-4b6b-9692-c0448bc54d5e",
   "metadata": {},
   "source": [
    "Chunking data into hDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae42fa5-b51b-4dfb-9486-c50346497af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your large .dta file (replace this with your actual file path)\n",
    "file_path = '../Data/dsws_variables_monthly_basic_2021.dta'\n",
    "\n",
    "# Directory to save the organized data by year and quarter\n",
    "output_dir = '../Data/dsws_Data/'  # Update this\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Define chunk size (e.g., 100,000 rows at a time)\n",
    "chunk_size = 100000\n",
    "\n",
    "# Function to extract the year and quarter from the data\n",
    "def extract_year_quarter(df, date_column):\n",
    "    df['year'] = pd.to_datetime(df[date_column]).dt.year\n",
    "    df['quarter'] = pd.to_datetime(df[date_column]).dt.to_period('Q')\n",
    "    \n",
    "    # Convert 'quarter' to a string to avoid issues with HDF5\n",
    "    df['quarter'] = df['quarter'].astype(str)\n",
    "    \n",
    "    # Return a new DataFrame with both columns at once to avoid fragmentation\n",
    "    return df\n",
    "\n",
    "# Read the .dta file in chunks\n",
    "chunk_iter = pd.read_stata(file_path, chunksize=chunk_size)\n",
    "\n",
    "# Process each chunk\n",
    "for i, chunk in enumerate(chunk_iter):\n",
    "    print(f'Processing chunk {i+1}...')\n",
    "    \n",
    "    # Extract year and quarter (assuming you have a date column like 'date')\n",
    "    chunk = extract_year_quarter(chunk, 'date')  # Update 'date' to your actual date column\n",
    "\n",
    "    # Group the chunk by year and quarter, and save it into separate HDF5 files\n",
    "    for (year, quarter), group in chunk.groupby(['year', 'quarter']):\n",
    "        # Create a subdirectory for each year\n",
    "        year_dir = os.path.join(output_dir, str(year))\n",
    "        os.makedirs(year_dir, exist_ok=True)\n",
    "\n",
    "        # Define the filename for the HDF5 file for each quarter\n",
    "        hdf5_filename = os.path.join(year_dir, f'{year}_Q{quarter[-1]}.h5')\n",
    "\n",
    "        # Define the key for the HDF5 file\n",
    "        hdf_key = f'year_{year}_Q{quarter[-1]}'\n",
    "        \n",
    "        # Open or create the HDF5 file and append the data using 'fixed' format\n",
    "        with pd.HDFStore(hdf5_filename, mode='a') as hdf_store:\n",
    "            hdf_store.put(hdf_key, group, format='fixed')  # Use 'fixed' format for better performance\n",
    "\n",
    "print(\"Data processing and saving to HDF5 files complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyenv)",
   "language": "python",
   "name": "pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4baa7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# data imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# # ML imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb718a0-ad66-4169-ab86-4cf8b3f1e780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34cbfa1b-b60d-4b86-941f-e70c6d141327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stock data\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = '../Data/dsws_Data/2021/'\n",
    "\n",
    "# Get all .pkl files within folder\n",
    "pickle_files = [f for f in os.listdir(folder_path) if f.endswith('.pkl')]\n",
    "\n",
    "cols = [\n",
    "    'month_id', 'fiscal_date', 'year_ws', 'FullName', 'isin', 'ibes_ticker',\n",
    "    'country', 'permno', 'cusip_8', 'cusip_9', 'region', 'siccd_numerical',\n",
    "    'fama_french_48', 'fama_french_12', 'ret_usd', 'date', 'year_id', 'month',\n",
    "    'day', 'monthyear', 'yearmonth', 'daymonthyear', 'yearmonthday', 'size',\n",
    "    'price', 'size_local', 'size_local_in_mio', 'interim_report', 'year_ff',\n",
    "    'rank_size', 'price_avg', 'siccd', 'sic1', 'sic2', 'sic3', 'ff48', 'ff12',\n",
    "    'ret_12_13', 'ret_3_12', 'ret_3_9', 'ret_37_136', 'ret_82_136', \n",
    "    'ret_49_70', 'ret_61_120', 'ret_121_180'\n",
    "]\n",
    "\n",
    "df_list = []  #  Init empty list for the dataframes\n",
    "\n",
    "# Loop through the pickle files and append them to a dataframe\n",
    "for file in pickle_files:\n",
    "    file_path = os.path.join(folder_path, file)  # Full path to the pickle file\n",
    "    df = pd.read_pickle(file_path)  # Read the pickle file into a dataframe\n",
    "    df_list.append(df[cols])  # Append the dataframe to the list\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "df_dta = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Create a new column 'quarter' that extracts the quarter number from 'fiscal_date'\n",
    "df_dta['Quarter'] = df_dta['fiscal_date'].dt.quarter\n",
    "\n",
    "# Insert the 'quarter' column right after the 'fiscal_date' column\n",
    "fiscal_date_index = df_dta.columns.get_loc('fiscal_date')  # Get index of 'fiscal_date'\n",
    "df_dta.insert(fiscal_date_index + 1, 'Quarter', df_dta.pop('Quarter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33b63914-ab19-4d8c-bd25-fd4cb37bb1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_id</th>\n",
       "      <th>fiscal_date</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>year_ws</th>\n",
       "      <th>FullName</th>\n",
       "      <th>isin</th>\n",
       "      <th>ibes_ticker</th>\n",
       "      <th>country</th>\n",
       "      <th>permno</th>\n",
       "      <th>cusip_8</th>\n",
       "      <th>...</th>\n",
       "      <th>ff48</th>\n",
       "      <th>ff12</th>\n",
       "      <th>ret_12_13</th>\n",
       "      <th>ret_3_12</th>\n",
       "      <th>ret_3_9</th>\n",
       "      <th>ret_37_136</th>\n",
       "      <th>ret_82_136</th>\n",
       "      <th>ret_49_70</th>\n",
       "      <th>ret_61_120</th>\n",
       "      <th>ret_121_180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>734.0</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>4</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Johnson and Johnson</td>\n",
       "      <td>US4781601046</td>\n",
       "      <td>@:JNJ</td>\n",
       "      <td>usa</td>\n",
       "      <td>22111.0</td>\n",
       "      <td>47816010</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0.115705</td>\n",
       "      <td>0.269904</td>\n",
       "      <td>0.175892</td>\n",
       "      <td>1.625825</td>\n",
       "      <td>0.666443</td>\n",
       "      <td>0.306768</td>\n",
       "      <td>1.135767</td>\n",
       "      <td>0.159502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>734.0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>4</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Hallmark Financial Services</td>\n",
       "      <td>US40624Q2030</td>\n",
       "      <td>@:HLFS</td>\n",
       "      <td>usa</td>\n",
       "      <td>75985.0</td>\n",
       "      <td>40624Q20</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.695345</td>\n",
       "      <td>-0.108911</td>\n",
       "      <td>0.031518</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>-0.834437</td>\n",
       "      <td>-0.004505</td>\n",
       "      <td>0.372315</td>\n",
       "      <td>-0.173570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>734.0</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Nordstrom</td>\n",
       "      <td>US6556641008</td>\n",
       "      <td>@:NOBE</td>\n",
       "      <td>usa</td>\n",
       "      <td>57817.0</td>\n",
       "      <td>65566410</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.451315</td>\n",
       "      <td>1.310952</td>\n",
       "      <td>1.288573</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>-0.010279</td>\n",
       "      <td>-0.278404</td>\n",
       "      <td>0.513789</td>\n",
       "      <td>0.273126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>734.0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>4</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Universal Health Services 'B'</td>\n",
       "      <td>US9139031002</td>\n",
       "      <td>@:UHSI</td>\n",
       "      <td>usa</td>\n",
       "      <td>79637.0</td>\n",
       "      <td>91390310</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.145871</td>\n",
       "      <td>0.258376</td>\n",
       "      <td>0.342232</td>\n",
       "      <td>3.411479</td>\n",
       "      <td>2.723388</td>\n",
       "      <td>-0.034230</td>\n",
       "      <td>1.592820</td>\n",
       "      <td>1.003297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>734.0</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Oxford Industries</td>\n",
       "      <td>US6914973093</td>\n",
       "      <td>@:OXM</td>\n",
       "      <td>usa</td>\n",
       "      <td>34948.0</td>\n",
       "      <td>69149730</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.301095</td>\n",
       "      <td>0.838330</td>\n",
       "      <td>0.504311</td>\n",
       "      <td>2.929364</td>\n",
       "      <td>1.824207</td>\n",
       "      <td>-0.223692</td>\n",
       "      <td>1.098642</td>\n",
       "      <td>-0.234777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   month_id fiscal_date  Quarter  year_ws                       FullName  \\\n",
       "0     734.0  2019-12-29        4   2019.0            Johnson and Johnson   \n",
       "1     734.0  2019-12-31        4   2019.0    Hallmark Financial Services   \n",
       "2     734.0  2020-02-01        1   2019.0                      Nordstrom   \n",
       "3     734.0  2019-12-31        4   2019.0  Universal Health Services 'B'   \n",
       "4     734.0  2020-02-01        1   2019.0              Oxford Industries   \n",
       "\n",
       "           isin ibes_ticker country   permno   cusip_8  ... ff48 ff12  \\\n",
       "0  US4781601046       @:JNJ     usa  22111.0  47816010  ...   13   10   \n",
       "1  US40624Q2030      @:HLFS     usa  75985.0  40624Q20  ...   45   11   \n",
       "2  US6556641008      @:NOBE     usa  57817.0  65566410  ...   42    9   \n",
       "3  US9139031002      @:UHSI     usa  79637.0  91390310  ...   11   10   \n",
       "4  US6914973093       @:OXM     usa  34948.0  69149730  ...   42    9   \n",
       "\n",
       "   ret_12_13  ret_3_12   ret_3_9  ret_37_136 ret_82_136  ret_49_70  \\\n",
       "0   0.115705  0.269904  0.175892    1.625825   0.666443   0.306768   \n",
       "1  -0.695345 -0.108911  0.031518    0.159947  -0.834437  -0.004505   \n",
       "2  -0.451315  1.310952  1.288573    0.880952  -0.010279  -0.278404   \n",
       "3  -0.145871  0.258376  0.342232    3.411479   2.723388  -0.034230   \n",
       "4  -0.301095  0.838330  0.504311    2.929364   1.824207  -0.223692   \n",
       "\n",
       "   ret_61_120  ret_121_180  \n",
       "0    1.135767     0.159502  \n",
       "1    0.372315    -0.173570  \n",
       "2    0.513789     0.273126  \n",
       "3    1.592820     1.003297  \n",
       "4    1.098642    -0.234777  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddc575a-3e90-42a6-a382-9f1631458d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month_id                    float32\n",
      "fiscal_date          datetime64[ns]\n",
      "year_ws                     float32\n",
      "FullName                     object\n",
      "isin                         object\n",
      "ibes_ticker                  object\n",
      "country                      object\n",
      "permno                      float64\n",
      "cusip_8                      object\n",
      "cusip_9                      object\n",
      "region                       object\n",
      "siccd_numerical             float64\n",
      "fama_french_48              float32\n",
      "fama_french_12              float32\n",
      "ret_usd                     float32\n",
      "date                 datetime64[ns]\n",
      "year_id                     float32\n",
      "month                       float32\n",
      "day                         float32\n",
      "monthyear                    object\n",
      "yearmonth                    object\n",
      "daymonthyear                 object\n",
      "yearmonthday                 object\n",
      "size                        float32\n",
      "price                       float32\n",
      "size_local                  float32\n",
      "size_local_in_mio           float32\n",
      "interim_report               object\n",
      "year_ff                     float32\n",
      "rank_size                   float32\n",
      "price_avg                   float32\n",
      "siccd                        object\n",
      "sic1                         object\n",
      "sic2                         object\n",
      "sic3                         object\n",
      "ff48                         object\n",
      "ff12                         object\n",
      "ret_12_13                   float32\n",
      "ret_3_12                    float32\n",
      "ret_3_9                     float32\n",
      "ret_37_136                  float32\n",
      "ret_82_136                  float32\n",
      "ret_49_70                   float32\n",
      "ret_61_120                  float32\n",
      "ret_121_180                 float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "result = df_dta.dtypes\n",
    "print(result)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0029ed-2b0f-4831-b018-9beb2a3ab17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 10X data\n",
    "\n",
    "folder_path = '../Data/10-X_C_2021/2021/'\n",
    "\n",
    "pickle_files = [f for f in os.listdir(folder_path) if f.endswith('.pkl')]\n",
    "\n",
    "df_list = []  #  Init empty list for the dataframes\n",
    "\n",
    "# Loop through the pickle files and append them to a dataframe\n",
    "for file in pickle_files:\n",
    "    file_path = os.path.join(folder_path, file)  # Full path to the pickle file\n",
    "    df = pd.read_pickle(file_path)  # Read the pickle file into a dataframe\n",
    "    df_list.append(df)  # Append the dataframe to the list\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "df_filings = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2afa69b3-6211-4c93-97a3-b1192698369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>SIC Code</th>\n",
       "      <th>Filing Type</th>\n",
       "      <th>Conformed Period of Report</th>\n",
       "      <th>Year</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Formatted Date</th>\n",
       "      <th>Item 1A - Risk Factors</th>\n",
       "      <th>Item 1C - Cybersecurity</th>\n",
       "      <th>Item 7 - MD&amp;A</th>\n",
       "      <th>Item 7A - Market Risk</th>\n",
       "      <th>Item 8 - Financial Statements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210104_10-K_edgar_data_1041588_0001041588-21...</td>\n",
       "      <td>Access-Power &amp; Co., Inc.</td>\n",
       "      <td>4813</td>\n",
       "      <td>10-K</td>\n",
       "      <td>20201231</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>ITEM 1a. RISK FACTORS  ACCR DOES NOT BELIEVE I...</td>\n",
       "      <td>None</td>\n",
       "      <td>ITEM 7.\\t MANAGEMENTS DISCUSSION AND ANALYSIS ...</td>\n",
       "      <td>ITEM 7a. QUANTITATIVE AND QUALITATIVE DISCLOSU...</td>\n",
       "      <td>ITEM 8.\\t FINANCIAL STATEMENTS AND SUPPLEMENTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20210104_10-K_edgar_data_1604930_0001493152-21...</td>\n",
       "      <td>Life Clips, Inc.</td>\n",
       "      <td>3861</td>\n",
       "      <td>10-K</td>\n",
       "      <td>20200630</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>Item  1A. Risk Factors   6</td>\n",
       "      <td>None</td>\n",
       "      <td>Item  7. Management s Discussion and Analysis ...</td>\n",
       "      <td>Item  7A. Quantitative and Qualitative Disclos...</td>\n",
       "      <td>Item  8. Consolidated Financial Statements and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20210104_10-Q_edgar_data_1098009_0001185185-20...</td>\n",
       "      <td>America Great Health</td>\n",
       "      <td>2834</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>20190331</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>ITEM 1A   Risk Factors   15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ITEM 2   Management s Discussion and Analysis ...</td>\n",
       "      <td>ITEM 3   Quantitative and Qualitative Disclosu...</td>\n",
       "      <td>ITEM 1   Condensed Consolidated Financial Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20210104_10-Q_edgar_data_1556179_0001104659-20...</td>\n",
       "      <td>Rocky Mountain Industrials, Inc.</td>\n",
       "      <td>7380</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>20190630</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>ITEM 1A.   RISK FACTORS   10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Item 2 - Management's Discussion and Analysis ...</td>\n",
       "      <td>ITEM 3.   QUANTITATIVE AND QUALITATIVE DISCLOS...</td>\n",
       "      <td>ITEM 1.   FINANCIAL STATEMENTS   5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20210104_10-Q_edgar_data_1556179_0001104659-20...</td>\n",
       "      <td>Rocky Mountain Industrials, Inc.</td>\n",
       "      <td>7380</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>20190930</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>ITEM 1A.   RISK FACTORS   10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Item 2 - Management's Discussion and Analysis ...</td>\n",
       "      <td>ITEM 3.   QUANTITATIVE AND QUALITATIVE DISCLOS...</td>\n",
       "      <td>ITEM 1.   FINANCIAL STATEMENTS   5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name  \\\n",
       "0  20210104_10-K_edgar_data_1041588_0001041588-21...   \n",
       "1  20210104_10-K_edgar_data_1604930_0001493152-21...   \n",
       "2  20210104_10-Q_edgar_data_1098009_0001185185-20...   \n",
       "3  20210104_10-Q_edgar_data_1556179_0001104659-20...   \n",
       "4  20210104_10-Q_edgar_data_1556179_0001104659-20...   \n",
       "\n",
       "                       Company Name SIC Code Filing Type  \\\n",
       "0          Access-Power & Co., Inc.     4813        10-K   \n",
       "1                  Life Clips, Inc.     3861        10-K   \n",
       "2              America Great Health     2834        10-Q   \n",
       "3  Rocky Mountain Industrials, Inc.     7380        10-Q   \n",
       "4  Rocky Mountain Industrials, Inc.     7380        10-Q   \n",
       "\n",
       "  Conformed Period of Report  Year  Quarter Formatted Date  \\\n",
       "0                   20201231  2020        4     2020-12-31   \n",
       "1                   20200630  2020        2     2020-06-30   \n",
       "2                   20190331  2019        1     2019-03-31   \n",
       "3                   20190630  2019        2     2019-06-30   \n",
       "4                   20190930  2019        3     2019-09-30   \n",
       "\n",
       "                              Item 1A - Risk Factors Item 1C - Cybersecurity  \\\n",
       "0  ITEM 1a. RISK FACTORS  ACCR DOES NOT BELIEVE I...                    None   \n",
       "1                         Item  1A. Risk Factors   6                    None   \n",
       "2                        ITEM 1A   Risk Factors   15                     NaN   \n",
       "3                       ITEM 1A.   RISK FACTORS   10                     NaN   \n",
       "4                       ITEM 1A.   RISK FACTORS   10                     NaN   \n",
       "\n",
       "                                       Item 7 - MD&A  \\\n",
       "0  ITEM 7.\\t MANAGEMENTS DISCUSSION AND ANALYSIS ...   \n",
       "1  Item  7. Management s Discussion and Analysis ...   \n",
       "2  ITEM 2   Management s Discussion and Analysis ...   \n",
       "3  Item 2 - Management's Discussion and Analysis ...   \n",
       "4  Item 2 - Management's Discussion and Analysis ...   \n",
       "\n",
       "                               Item 7A - Market Risk  \\\n",
       "0  ITEM 7a. QUANTITATIVE AND QUALITATIVE DISCLOSU...   \n",
       "1  Item  7A. Quantitative and Qualitative Disclos...   \n",
       "2  ITEM 3   Quantitative and Qualitative Disclosu...   \n",
       "3  ITEM 3.   QUANTITATIVE AND QUALITATIVE DISCLOS...   \n",
       "4  ITEM 3.   QUANTITATIVE AND QUALITATIVE DISCLOS...   \n",
       "\n",
       "                       Item 8 - Financial Statements  \n",
       "0  ITEM 8.\\t FINANCIAL STATEMENTS AND SUPPLEMENTA...  \n",
       "1  Item  8. Consolidated Financial Statements and...  \n",
       "2  ITEM 1   Condensed Consolidated Financial Stat...  \n",
       "3                 ITEM 1.   FINANCIAL STATEMENTS   5  \n",
       "4                 ITEM 1.   FINANCIAL STATEMENTS   5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e7e625d-24f5-434c-b5c2-e1897db6132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge them\n",
    "merged_df = pd.merge(df_dta, df_filings, left_on=['Quarter', 'FullName', 'siccd'], \n",
    "                     right_on=['Quarter', 'Company Name', 'SIC Code'], \n",
    "                     how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2faac742-252d-4776-ae3e-3a6fcb5d7f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_id</th>\n",
       "      <th>fiscal_date</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>year_ws</th>\n",
       "      <th>FullName</th>\n",
       "      <th>isin</th>\n",
       "      <th>ibes_ticker</th>\n",
       "      <th>country</th>\n",
       "      <th>permno</th>\n",
       "      <th>cusip_8</th>\n",
       "      <th>...</th>\n",
       "      <th>SIC Code</th>\n",
       "      <th>Filing Type</th>\n",
       "      <th>Conformed Period of Report</th>\n",
       "      <th>Year</th>\n",
       "      <th>Formatted Date</th>\n",
       "      <th>Item 1A - Risk Factors</th>\n",
       "      <th>Item 1C - Cybersecurity</th>\n",
       "      <th>Item 7 - MD&amp;A</th>\n",
       "      <th>Item 7A - Market Risk</th>\n",
       "      <th>Item 8 - Financial Statements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>734.0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>4</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>MGM Resorts International</td>\n",
       "      <td>US5529531015</td>\n",
       "      <td>@:MGMG</td>\n",
       "      <td>usa</td>\n",
       "      <td>11891.0</td>\n",
       "      <td>55295310</td>\n",
       "      <td>...</td>\n",
       "      <td>7011</td>\n",
       "      <td>10-K</td>\n",
       "      <td>20201231</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>Item 1A.     Risk Factors     14</td>\n",
       "      <td>None</td>\n",
       "      <td>Item 7.     Management's Discussion and Analys...</td>\n",
       "      <td>Item 7A.     Quantitative and Qualitative Disc...</td>\n",
       "      <td>Item 8.     Financial Statements and Supplemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>734.0</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>2</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Kentucky First Federal Bancorp</td>\n",
       "      <td>US4912921081</td>\n",
       "      <td>@:KFFB</td>\n",
       "      <td>usa</td>\n",
       "      <td>90636.0</td>\n",
       "      <td>49129210</td>\n",
       "      <td>...</td>\n",
       "      <td>6035</td>\n",
       "      <td>10-K</td>\n",
       "      <td>20210630</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>Item  1A.   Risk  Factors     17</td>\n",
       "      <td>None</td>\n",
       "      <td>Item  7.   Management s  Discussion and Analys...</td>\n",
       "      <td>Item  7A.   Quantitative  and Qualitative Disc...</td>\n",
       "      <td>Item  8.   Financial  Statements and Supplemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>734.0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>4</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Unum Group</td>\n",
       "      <td>US91529Y1064</td>\n",
       "      <td>@:UNM</td>\n",
       "      <td>usa</td>\n",
       "      <td>71175.0</td>\n",
       "      <td>91529Y10</td>\n",
       "      <td>...</td>\n",
       "      <td>6321</td>\n",
       "      <td>10-K</td>\n",
       "      <td>20201231</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>Item 1A.   Risk Factors   21   Item 1B.   Unre...</td>\n",
       "      <td>None</td>\n",
       "      <td>Item 7.   Management's Discussion and Analysis...</td>\n",
       "      <td>Item 7A.   Quantitative and Qualitative Disclo...</td>\n",
       "      <td>Item 8.   Financial Statements and Supplementa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>734.0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>4</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Oak Valley Bancorp</td>\n",
       "      <td>US6718071052</td>\n",
       "      <td>@:OVYB</td>\n",
       "      <td>usa</td>\n",
       "      <td>92878.0</td>\n",
       "      <td>67180710</td>\n",
       "      <td>...</td>\n",
       "      <td>6022</td>\n",
       "      <td>10-K</td>\n",
       "      <td>20201231</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>ITEM 1A -   RISK FACTORS   19</td>\n",
       "      <td>None</td>\n",
       "      <td>ITEM 7 -   MANAGEMENT S DISCUSSION AND ANALYSI...</td>\n",
       "      <td>ITEM 7A -   QUANTITATIVE AND QUALITATIVE DISCL...</td>\n",
       "      <td>ITEM 8 -   FINANCIAL STATEMENTS AND SUPPLEMENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>734.0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>4</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Bank of Marin Bancorp</td>\n",
       "      <td>US0634251021</td>\n",
       "      <td>@:BA3T</td>\n",
       "      <td>usa</td>\n",
       "      <td>87476.0</td>\n",
       "      <td>06342510</td>\n",
       "      <td>...</td>\n",
       "      <td>6022</td>\n",
       "      <td>10-K</td>\n",
       "      <td>20201231</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>ITEM 1A. RISK FACTORS Page- 12   ITEM 1B. UNRE...</td>\n",
       "      <td>None</td>\n",
       "      <td>ITEM 7. MANAGEMENT'S DISCUSSION AND ANALYSIS O...</td>\n",
       "      <td>ITEM 7A. QUANTITATIVE AND QUALITATIVE DISCLOSU...</td>\n",
       "      <td>ITEM 8. FINANCIAL STATEMENTS AND SUPPLEMENTARY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   month_id fiscal_date  Quarter  year_ws                        FullName  \\\n",
       "0     734.0  2019-12-31        4   2019.0       MGM Resorts International   \n",
       "1     734.0  2020-06-30        2   2020.0  Kentucky First Federal Bancorp   \n",
       "2     734.0  2019-12-31        4   2019.0                      Unum Group   \n",
       "3     734.0  2019-12-31        4   2019.0              Oak Valley Bancorp   \n",
       "4     734.0  2019-12-31        4   2019.0           Bank of Marin Bancorp   \n",
       "\n",
       "           isin ibes_ticker country   permno   cusip_8  ... SIC Code  \\\n",
       "0  US5529531015      @:MGMG     usa  11891.0  55295310  ...     7011   \n",
       "1  US4912921081      @:KFFB     usa  90636.0  49129210  ...     6035   \n",
       "2  US91529Y1064       @:UNM     usa  71175.0  91529Y10  ...     6321   \n",
       "3  US6718071052      @:OVYB     usa  92878.0  67180710  ...     6022   \n",
       "4  US0634251021      @:BA3T     usa  87476.0  06342510  ...     6022   \n",
       "\n",
       "  Filing Type  Conformed Period of Report  Year  Formatted Date  \\\n",
       "0        10-K                    20201231  2020      2020-12-31   \n",
       "1        10-K                    20210630  2021      2021-06-30   \n",
       "2        10-K                    20201231  2020      2020-12-31   \n",
       "3        10-K                    20201231  2020      2020-12-31   \n",
       "4        10-K                    20201231  2020      2020-12-31   \n",
       "\n",
       "                              Item 1A - Risk Factors Item 1C - Cybersecurity  \\\n",
       "0                   Item 1A.     Risk Factors     14                    None   \n",
       "1                   Item  1A.   Risk  Factors     17                    None   \n",
       "2  Item 1A.   Risk Factors   21   Item 1B.   Unre...                    None   \n",
       "3                      ITEM 1A -   RISK FACTORS   19                    None   \n",
       "4  ITEM 1A. RISK FACTORS Page- 12   ITEM 1B. UNRE...                    None   \n",
       "\n",
       "                                       Item 7 - MD&A  \\\n",
       "0  Item 7.     Management's Discussion and Analys...   \n",
       "1  Item  7.   Management s  Discussion and Analys...   \n",
       "2  Item 7.   Management's Discussion and Analysis...   \n",
       "3  ITEM 7 -   MANAGEMENT S DISCUSSION AND ANALYSI...   \n",
       "4  ITEM 7. MANAGEMENT'S DISCUSSION AND ANALYSIS O...   \n",
       "\n",
       "                               Item 7A - Market Risk  \\\n",
       "0  Item 7A.     Quantitative and Qualitative Disc...   \n",
       "1  Item  7A.   Quantitative  and Qualitative Disc...   \n",
       "2  Item 7A.   Quantitative and Qualitative Disclo...   \n",
       "3  ITEM 7A -   QUANTITATIVE AND QUALITATIVE DISCL...   \n",
       "4  ITEM 7A. QUANTITATIVE AND QUALITATIVE DISCLOSU...   \n",
       "\n",
       "                       Item 8 - Financial Statements  \n",
       "0  Item 8.     Financial Statements and Supplemen...  \n",
       "1  Item  8.   Financial  Statements and Supplemen...  \n",
       "2  Item 8.   Financial Statements and Supplementa...  \n",
       "3  ITEM 8 -   FINANCIAL STATEMENTS AND SUPPLEMENT...  \n",
       "4  ITEM 8. FINANCIAL STATEMENTS AND SUPPLEMENTARY...  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3cb96-edcf-438a-9bb2-6ff5eaf69386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin to create magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46131bbc-e74b-4282-b2e8-09cf3d63368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT pre-trained model for sentiment analysis\n",
    "tokenizer = BertTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf7608-2740-44f8-9cb3-25179b43945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = '../Data/10-X_C_2021/2021/QTR1/'  \n",
    "stock_data_path = '../Data/dsws_Data/2021/' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790846ec-d6ba-4aca-b451-1f4f49cbac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_and_sic(text_file_path):\n",
    "    with open(text_file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Extract the content between <SEC-Header> and </SEC-Header>\n",
    "    header_content = re.search(r'<SEC-Header>(.*?)</SEC-Header>', data, re.DOTALL)\n",
    "    if header_content:\n",
    "        header_data = header_content.group(1)\n",
    "        \n",
    "        # Extract company name\n",
    "        company_match = re.search(r'COMPANY CONFORMED NAME:\\s+(.+)', header_data)\n",
    "        company_name = company_match.group(1).strip() if company_match else \"Unknown\"\n",
    "\n",
    "        # Extract SIC code (Assuming the format: [xxxx])\n",
    "        sic_match = re.search(r'STANDARD INDUSTRIAL CLASSIFICATION:.*\\[(\\d+)\\]', header_data)\n",
    "        sic_code = sic_match.group(1).strip() if sic_match else \"Unknown\"\n",
    "\n",
    "        print(f\"Company Name: {company_name}, SIC Code: {sic_code}\")\n",
    "        return company_name, sic_code\n",
    "    else:\n",
    "        print(\"SEC-Header not found in the document.\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage with your text file\n",
    "company_name, sic_code = extract_company_and_sic('../Data/2021/20210104_10-K_edgar_data_1041588_0001041588-21-000001.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd4681-af07-42c1-89c2-bf79432fcda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec5e4f-7135-4264-8a9b-a1a3007623df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5a60f-4a97-43a0-910e-0456e1829019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91677d8-03f0-4e37-9af8-f632f8631bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fdb565-66d5-4102-aca9-a45e833c5160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96139292-2e31-4db8-9842-73b846448c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb113f9-4b93-4a38-aa7a-5ae042d438fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74f818-d18b-4f97-bc38-67457c40ef56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f6fc4-700b-418e-8b45-061bfc575bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b337134-17da-42d6-8ba7-f91ab0b7512a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b379f-0470-41db-9709-dd7e7694e1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8e5ab-ee7e-450f-8fcb-8d9cf06c5f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755965e-63cb-4728-9a0b-4aa087c5d338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0228f-316f-434a-9f88-857f0391e6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd398b-c351-4e8f-90c0-b3f4515d54d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b981d59-0afc-41bd-be45-c5e4e5a54e18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to extract COMPANY CONFORMED NAME, SIC code, and main text (excluding <SEC-Header>)\n",
    "def process_filing_file(file_path):\n",
    "    ''' \n",
    "    This function ... TBD\n",
    "    \n",
    "    '''\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Extract the content between <SEC-Header> and </SEC-Header>\n",
    "    header_content = re.search(r'<SEC-Header>(.*?)</SEC-Header>', data, re.DOTALL)\n",
    "    \n",
    "    if header_content:\n",
    "        header_data = header_content.group(1)\n",
    "\n",
    "        # Ensure the filing type is either 10-Q or 10-K\n",
    "        filing_type_match = re.search(r'CONFORMED SUBMISSION TYPE:\\s+(10-Q|10-K)', header_data)\n",
    "        if filing_type_match:\n",
    "            filing_type = filing_type_match.group(1)\n",
    "\n",
    "            # Extract company name\n",
    "            company_match = re.search(r'COMPANY CONFORMED NAME:\\s+(.+)', header_data)\n",
    "            company_name = company_match.group(1).strip() if company_match else \"Unknown\"\n",
    "\n",
    "            # Extract SIC code (Assuming the format: [xxxx])\n",
    "            sic_match = re.search(r'STANDARD INDUSTRIAL CLASSIFICATION:.*\\[(\\d+)\\]', header_data)\n",
    "            sic_code = sic_match.group(1).strip() if sic_match else \"Unknown\"\n",
    "\n",
    "            # Extract Conformed Period of Report\n",
    "            period_match = re.search(r'CONFORMED PERIOD OF REPORT:\\s+(\\d{8})', header_data)\n",
    "            period = period_match.group(1).strip() if period_match else None\n",
    "\n",
    "            #  Extract year, quarter, and formatted date\n",
    "            if period:\n",
    "                # Use pandas to handle date parsing and quarter extraction\n",
    "                date = pd.to_datetime(period, format='%Y%m%d')\n",
    "                year = date.year\n",
    "                quarter = date.quarter\n",
    "                formatted_date = date.strftime('%Y-%m-%d')  # Format the date as YYYY-MM-DD\n",
    "            else:\n",
    "                year = None\n",
    "                quarter = None\n",
    "                formatted_date = None\n",
    "            # Remove <SEC-Header> content \n",
    "            cleaned_text = re.sub(r'<SEC-Header>.*?</SEC-Header>', '', data, flags=re.DOTALL).strip()\n",
    "\n",
    "            # Extract different items (sections of the filing)\n",
    "\n",
    "            # Extract items based on filing type due to differences in 10K and 10Q filing structure\n",
    "            if filing_type == '10-Q':\n",
    "                item_1 = extract_item_section(cleaned_text, '1')  # Item 1 - Financial Statements\n",
    "                item_2 = extract_item_section(cleaned_text, '2')  # Item 2 - MD&A\n",
    "                item_3 = extract_item_section(cleaned_text, '3')  # Item 3 - Market Risk\n",
    "                item_1a = extract_item_section(cleaned_text, '1A')  # Item 1A - Risk Factors\n",
    "                return {\n",
    "                    'File Name': os.path.basename(file_path),\n",
    "                    'Company Name': company_name,\n",
    "                    'SIC Code': sic_code,\n",
    "                    'Filing Type': filing_type,\n",
    "                    'Conformed Period of Report': period,\n",
    "                    'Year': year,\n",
    "                    'Quarter': quarter,\n",
    "                    'Formatted Date': formatted_date,\n",
    "                    'Item 8 - Financial Statements': item_1,\n",
    "                    'Item 7 - MD&A': item_2,\n",
    "                    'Item 7A - Market Risk': item_3,\n",
    "                    'Item 1A - Risk Factors': item_1a\n",
    "                }\n",
    "            elif filing_type == '10-K':\n",
    "                #item_1 = extract_item_section(cleaned_text, '1')  # Item 1 - Business\n",
    "                item_1a = extract_item_section(cleaned_text, '1A')  # Item 1A - Risk Factors\n",
    "                item_1c = extract_item_section(cleaned_text, '1C')  # Item 1C - Cybersecurity\n",
    "                item_7 = extract_item_section(cleaned_text, '7')  # Item 7 - MD&A\n",
    "                item_7a = extract_item_section(cleaned_text, '7A')  # Item 7A - Market Risk\n",
    "                item_8 = extract_item_section(cleaned_text, '8')  # Item 8 - Financial Statements\n",
    "                return {\n",
    "                    'File Name': os.path.basename(file_path),\n",
    "                    'Company Name': company_name,\n",
    "                    'SIC Code': sic_code,\n",
    "                    'Filing Type': filing_type,\n",
    "                    'Conformed Period of Report': period,\n",
    "                    'Year': year,\n",
    "                    'Quarter': quarter,\n",
    "                    'Formatted Date': formatted_date,\n",
    "                    #'Item 1 - Business': item_1,\n",
    "                    'Item 1A - Risk Factors': item_1a,\n",
    "                    'Item 1C - Cybersecurity': item_1c,\n",
    "                    'Item 7 - MD&A': item_7,\n",
    "                    'Item 7A - Market Risk': item_7a,\n",
    "                    'Item 8 - Financial Statements': item_8\n",
    "                }   \n",
    "        else:\n",
    "            print(f\"File {file_path} is neither a 10-Q nor 10-K filing.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"SEC-Header not found in the document {file_path}.\")\n",
    "        return None\n",
    "\n",
    "# Extract the sections based on item tag used in SEC filings using a matching method\n",
    "def extract_item_section(text, item_number):\n",
    "    # Matches text from Item X until the next item or end of text\n",
    "    #pattern = fr\"Item {item_number}[\\s\\S]*?(?=Item \\d|\\Z)\"\n",
    "    #pattern = fr\"Item\\s*{item_number}[\\.\\s\\r\\n]+[\\s\\S]*?(?=\\nItem\\s*\\d|\\Z)\"\n",
    "    \n",
    "    pattern = fr\"ITEM\\s*{item_number}[\\.\\s\\r\\n]+[\\s\\S]*?(?=\\nITEM\\s*\\d|\\Z)\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        # Get the text, split by the last newline, and replace other newlines with spaces\n",
    "        #section_text = match.group(0)\n",
    "        #section_text = section_text.rsplit('\\n', 1)[0]  # Keep everything before the last newline\n",
    "        #section_text = section_text.replace('\\n', ' ')  # Replace all other newlines with spaces\n",
    "        section_text = match.group(0).replace('\\n', ' ').replace('\\r', ' ')\n",
    "        return section_text.strip()\n",
    "    return None\n",
    "    \n",
    "# Recursively look through all text files in folder and process them\n",
    "def process_all_filings(folder_path):\n",
    "    ''' This function recursively looks through all files and recursively processes them into \n",
    "    dataframes using process_filing_file function. \n",
    "    '''\n",
    "    data = []\n",
    "\n",
    "   # Traverse through all subdolders and files\n",
    "    for dirpath, _, filenames in os.walk(folder_path):\n",
    "        for file_name in filenames:\n",
    "            if file_name.endswith('.txt'):  # Using only .txt files\n",
    "                file_path = os.path.join(dirpath, file_name)\n",
    "                filing_data = process_filing_file(file_path)\n",
    "\n",
    "                if filing_data:\n",
    "                    data.append(filing_data) # Add to data list\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "folder_path = '../Data/10-X_C_2021/2021/'\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Process all the filings\n",
    "df_filings = process_all_filings(folder_path)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the time taken\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Time taken to process the filings: {execution_time} seconds\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_filings.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b804c1-833d-4309-909a-be03a36d42dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filings.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a516dd-cd9c-48c5-8e0b-978a6034665b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df46bdad-deb7-4a15-9dbd-d0b3872b8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10X.to_csv('10X_filings_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f1372-1cb9-477b-9c20-0b6b252ac912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c9cb9f1-a012-4775-9906-3772783f4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentiment analysis pipeline using the FinBERT model\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def tags_filter(text, start_tag, end_tag):\n",
    "    pattern = f'{start_tag}(.*?){end_tag}'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)  # re.DOTALL to match across newlines\n",
    "    return matches\n",
    "\n",
    "# Extract the stock ticker \n",
    "def stock_symbols(text):\n",
    "    # Regex for stock symbols (e.g., AAPL, TSLA)\n",
    "    return re.findall(r'\\b[A-Z]{2,5}\\b', text)\n",
    "\n",
    "# Function to clean and process document text\n",
    "def clean_text(text):\n",
    "    # Remove unnecessary whitespaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Function to perform sentiment analysis on each section related to a company\n",
    "def analyze_sentiment(text, stock_symbols):\n",
    "    results = []\n",
    "    # Split text into sentences or chunks for analysis\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    \n",
    "    # Analyze each sentence for stock-related mentions\n",
    "    for sentence in sentences:\n",
    "        for symbol in stock_symbols:\n",
    "            if symbol in sentence:\n",
    "                # Perform sentiment analysis on the sentence\n",
    "                sentiment = nlp(sentence)\n",
    "                results.append({\"Company\": symbol, \"Text\": sentence, \"Sentiment\": sentiment[0]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e31f78-2fd6-4d13-bdcb-1f8aba1a0836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ge78zoz\\pyenv\\lib\\site-packages\\dask\\array\\numpy_compat.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.divide(x1, x2, out)\n",
      "C:\\Users\\ge78zoz\\pyenv\\lib\\site-packages\\dask\\array\\numpy_compat.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.divide(x1, x2, out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           month_id          fiscal_date       year_ws  firm_loeschen  \\\n",
      "count  1.034538e+07             10345379  1.034538e+07   1.034538e+07   \n",
      "min    2.460000e+02  1980-01-03 00:00:00  1.980000e+03   0.000000e+00   \n",
      "25%    4.990000e+02  2000-12-31 00:00:00  2.000000e+03   0.000000e+00   \n",
      "50%    5.980000e+02  2008-03-31 00:00:00  2.008000e+03   0.000000e+00   \n",
      "75%    6.630000e+02  2014-03-31 00:00:00  2.014000e+03   0.000000e+00   \n",
      "max    7.370000e+02  2020-12-31 00:00:00  2.021000e+03   1.000000e+00   \n",
      "mean   5.811437e+02                  NaN  2.006784e+03   7.709123e-02   \n",
      "std    1.082745e+02                  NaN  9.009677e+00   2.667361e-01   \n",
      "\n",
      "             permno  dummy_in_crsp  siccd_numerical  fama_french_48  \\\n",
      "count  1.831076e+06      1831076.0     1.033028e+07    1.031494e+07   \n",
      "min    1.000100e+04            1.0     1.000000e+02    1.000000e+00   \n",
      "25%    5.671000e+04            1.0     2.911000e+03    1.800000e+01   \n",
      "50%    8.046200e+04            1.0     3.999000e+03    3.400000e+01   \n",
      "75%    8.973500e+04            1.0     6.211000e+03    4.200000e+01   \n",
      "max    9.343600e+04            1.0     9.999000e+03    4.800000e+01   \n",
      "mean   6.190619e+04            1.0     4.353076e+03    2.861419e+01   \n",
      "std    2.807142e+04            0.0     2.079081e+03    1.309514e+01   \n",
      "\n",
      "       d_financial_firm  fama_french_12  ...   sample_2   mv_pr_month  \\\n",
      "count      1.034538e+07    1.034538e+07  ...  8884731.0  1.027021e+07   \n",
      "min        0.000000e+00    1.000000e+00  ...        1.0  0.000000e+00   \n",
      "25%        0.000000e+00    4.000000e+00  ...        1.0  5.682000e+01   \n",
      "50%        0.000000e+00    9.000000e+00  ...        1.0  1.878300e+02   \n",
      "75%        0.000000e+00    1.100000e+01  ...        1.0  6.607800e+02   \n",
      "max        1.000000e+00    1.200000e+01  ...        1.0  2.232280e+06   \n",
      "mean       1.506340e-01    7.487372e+00  ...        1.0  1.406717e+03   \n",
      "std        3.576918e-01    3.858782e+00  ...        0.0  9.757139e+03   \n",
      "\n",
      "           ln_price     iadj_size           ipo firm_size_mio  \\\n",
      "count  1.034504e+07  1.034538e+07  1.034538e+07  1.027021e+07   \n",
      "min   -9.210340e+00 -4.937254e+04  0.000000e+00  0.000000e+00   \n",
      "25%    3.713566e-01 -4.068500e+01  0.000000e+00  5.682000e+01   \n",
      "50%    1.962992e+00  0.000000e+00  0.000000e+00  1.878300e+02   \n",
      "75%    3.243568e+00  4.340113e+02  0.000000e+00  6.607800e+02   \n",
      "max    2.113271e+01  2.284416e+06  1.000000e+00  2.232280e+06   \n",
      "mean   8.021078e-01  1.223420e+03  1.494865e-01  1.406717e+03   \n",
      "std    2.388169e+00  9.791702e+03  3.565674e-01  9.757139e+03   \n",
      "\n",
      "       nyse_size_decile size_portfolio_ff  size_portfolio        res_ac  \n",
      "count      1.027021e+07      1.027021e+07    9.968462e+06  9.879262e+06  \n",
      "min        1.000000e+00      1.000000e+00    1.000000e+00 -4.273243e+00  \n",
      "25%        1.000000e+00      1.000000e+00    1.000000e+00 -1.422864e-01  \n",
      "50%        2.000000e+00      1.000000e+00    1.000000e+00  0.000000e+00  \n",
      "75%        6.000000e+00      1.000000e+00    1.000000e+00  5.917537e-01  \n",
      "max        1.000000e+01      2.000000e+00    2.000000e+00  3.808630e+00  \n",
      "mean       2.375644e+00      1.120930e+00    1.200681e+00  2.561182e-02  \n",
      "std        2.293160e+00      3.260456e-01    4.005097e-01  7.227393e-01  \n",
      "\n",
      "[8 rows x 793 columns]\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define the file path\n",
    "file_path = '../Data/2021/2021_Q1.dta'\n",
    "\n",
    "# Define the chunk size for reading the .dta file\n",
    "chunk_size = 100000  # Adjust based on available memory\n",
    "\n",
    "# Create an empty list to hold the chunks\n",
    "chunk_list = []\n",
    "\n",
    "# Read the .dta file in chunks using pandas\n",
    "for chunk in pd.read_stata(file_path, chunksize=chunk_size):\n",
    "    # Append each chunk to the list (you could process each chunk here as well)\n",
    "    chunk_list.append(chunk)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "# Note: This is still pandas but allows us to load the large file in chunks\n",
    "df_pandas = pd.concat(chunk_list, ignore_index=True)\n",
    "\n",
    "# Now convert the Pandas DataFrame into a Dask DataFrame\n",
    "df_dask = dd.from_pandas(df_pandas, npartitions=10)\n",
    "\n",
    "# Process the Dask DataFrame, for example, getting a summary statistic\n",
    "summary = df_dask.describe().compute()  # .compute() triggers actual computation\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23042b3c-f36f-4c91-9860-39c9d61d52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 10-K report data from file\n",
    "file_path = '../Data/10-X_C_2023/2023/QTR1/20230103_10-K_edgar_data_1487931_0001477932-23-000012.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    document_text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98bbd8c8-a0bc-4b31-9f0d-7fba788842f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(document_text)\n",
    "\n",
    "# start_tag = \"<ITEM 1A>\"  \n",
    "# end_tag = \"</ITEM 1A>\"  \n",
    "# tagged_texts = tags_filter(cleaned_text, start_tag, end_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc46e3e1-0fbb-4d12-8e6a-1f44441106dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m stock_symbols \u001b[38;5;241m=\u001b[39m extract_stock_symbols(cleaned_text)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 3: Perform sentiment analysis for sentences containing stock symbols\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m sentiment_results \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstock_symbols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Step 4: Create a DataFrame for better visualization of results\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df_sentiment \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sentiment_results)\n",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m, in \u001b[0;36manalyze_sentiment\u001b[1;34m(text, stock_symbols)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m symbol \u001b[38;5;129;01min\u001b[39;00m stock_symbols:\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m symbol \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;66;03m# Perform sentiment analysis on the sentence\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m             sentiment \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m             results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompany\u001b[39m\u001b[38;5;124m\"\u001b[39m: symbol, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentence, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentiment[\u001b[38;5;241m0\u001b[39m]})\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[1;32m--> 156\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\pipelines\\base.py:1268\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1261\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         )\n\u001b[0;32m   1266\u001b[0m     )\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\pipelines\\base.py:1275\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1274\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1275\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1276\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\pipelines\\base.py:1175\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1174\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1175\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1176\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    186\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1696\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1694\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1704\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1710\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\pyenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Extract stock symbols from the text (you can hardcode the company symbols if needed)\n",
    "stock_symbols = extract_stock_symbols(cleaned_text)\n",
    "\n",
    "# Step 3: Perform sentiment analysis for sentences containing stock symbols\n",
    "sentiment_results = analyze_sentiment(cleaned_text, stock_symbols)\n",
    "\n",
    "# Step 4: Create a DataFrame for better visualization of results\n",
    "df_sentiment = pd.DataFrame(sentiment_results)\n",
    "\n",
    "# Display the first few rows of sentiment results\n",
    "print(df_sentiment.head())\n",
    "\n",
    "# Step 5: (Optional) Save the results to a CSV for later analysis\n",
    "df_sentiment.to_csv('sentiment_analysis_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89dedc-0e28-4edb-9626-2fdfb50e9a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyenv)",
   "language": "python",
   "name": "pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
